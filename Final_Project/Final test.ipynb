{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from ghostnet.ghost_net import ghost_net\n",
    "# from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# 检验GPU是否可用\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoad(path1, path2):\n",
    "    paths1 = os.listdir(path1)\n",
    "    paths2 = os.listdir(path2)\n",
    "    dataset = []\n",
    "    labelset = []\n",
    "    \n",
    "    for file in paths1:\n",
    "        if file.find('mhd')>=0:\n",
    "            filepath = os.path.join(path1, file)\n",
    "            data = sitk.GetArrayFromImage(sitk.ReadImage(filepath))\n",
    "            data = data.reshape((1,256,256))\n",
    "            dataset.append(data)\n",
    "            labelset.append(1)\n",
    "            \n",
    "    for file in paths2:\n",
    "        if file.find('mhd')>=0:\n",
    "            filepath = os.path.join(path2, file)\n",
    "            data = sitk.GetArrayFromImage(sitk.ReadImage(filepath))\n",
    "            data = data.reshape((1,256,256))\n",
    "            dataset.append(data)\n",
    "            labelset.append(0)\n",
    "        \n",
    "    dataset = np.array(dataset)\n",
    "    labelset = np.array(labelset)\n",
    "    return dataset, labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(378, 1, 256, 256)\n",
      "(50, 1, 256, 256)\n",
      "(378, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label = dataLoad('Training/Diseased', 'Training/Healthy')\n",
    "test_data, test_label = dataLoad('Testing1/Diseased', 'Testing1/Healthy')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "train_data = train_data.reshape((378,256,256,1))\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data augmentation\n",
    "datagen = ImageDataGenerator(horizontal_flip = True)\n",
    "\n",
    "for X_batch, y_batch in datagen.flow(train_data.reshape((378,256,256)), train_label, batch_size = 9):\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(X_batch[i])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pca_data = []\n",
    "for i in range(378):\n",
    "    pca_data.append(train_data[i][0].flatten())\n",
    "pca = PCA(n_components=0.5)   \n",
    "pca.fit(pca_data)\n",
    "pca_data = pca.transform(pca_data)\n",
    "print(pca_data.shape)\n",
    "train_data = pca.inverse_transform(pca_data).reshape((378,1,256,256))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import  tensorflow as  tf\n",
    "\n",
    "#加载cifar-10数据集\n",
    "def dataLoad():\n",
    "    (train_data, train_label), (test_data, test_label) =  tf.keras.datasets.cifar10.load_data()\n",
    "    #数据集进行归一化\n",
    "    train_data=train_data/255\n",
    "    test_data=test_data/255\n",
    "    #将标签数据集从数组类型array修改成整形类型int\n",
    "    train_label.astype(np.int)\n",
    "    test_label.astype(np.int)\n",
    "    return (train_data, train_label), (test_data, test_label)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(train_data, train_label), (test_data, test_label) = dataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape((378,1,256,256))\n",
    "test_data = test_data.reshape((-1,1,256,256))\n",
    "train_data = torch.FloatTensor(train_data)\n",
    "test_data = torch.FloatTensor(test_data)\n",
    "train_label = torch.LongTensor(train_label)\n",
    "test_label = torch.LongTensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    # 初始化\n",
    "    def __init__(self, pics, lable):\n",
    "        # 读入数据\n",
    "        self.data = pics\n",
    "        self.lable = lable\n",
    "        \n",
    "    # 返回df的长度\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # 获取第idx+1列的数据\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx],self.lable[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset_train = MyDataset(train_data, train_label)\n",
    "my_dataset_test = MyDataset(test_data, test_label)\n",
    "train_loader = DataLoader(dataset = my_dataset_train, batch_size = 8, shuffle = True)\n",
    "test_loader = DataLoader(dataset = my_dataset_test, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ghost_net(width_mult=1.0,num_classes=2).to(device)\n",
    "# model = Net().to(device)\n",
    "# optimizer = optim.Adam(model.parameters(),lr=0.001)  # 优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/48], Loss: 0.2948\n",
      "Epoch [1/10], Step [20/48], Loss: 0.3067\n",
      "Epoch [1/10], Step [30/48], Loss: 0.2720\n",
      "Epoch [1/10], Step [40/48], Loss: 0.2631\n",
      "31\n",
      "Accuracy of the network on the 50 test images: 62.0 %\n",
      "Epoch [2/10], Step [10/48], Loss: 0.0915\n",
      "Epoch [2/10], Step [20/48], Loss: 0.0733\n",
      "Epoch [2/10], Step [30/48], Loss: 0.1310\n",
      "Epoch [2/10], Step [40/48], Loss: 0.1356\n",
      "32\n",
      "Accuracy of the network on the 50 test images: 64.0 %\n",
      "Epoch [3/10], Step [10/48], Loss: 0.1418\n",
      "Epoch [3/10], Step [20/48], Loss: 0.1030\n",
      "Epoch [3/10], Step [30/48], Loss: 0.1046\n",
      "Epoch [3/10], Step [40/48], Loss: 0.0935\n",
      "37\n",
      "Accuracy of the network on the 50 test images: 74.0 %\n",
      "Epoch [4/10], Step [10/48], Loss: 0.0307\n",
      "Epoch [4/10], Step [20/48], Loss: 0.0214\n",
      "Epoch [4/10], Step [30/48], Loss: 0.0278\n",
      "Epoch [4/10], Step [40/48], Loss: 0.0240\n",
      "32\n",
      "Accuracy of the network on the 50 test images: 64.0 %\n",
      "Epoch [5/10], Step [10/48], Loss: 0.0085\n",
      "Epoch [5/10], Step [20/48], Loss: 0.0167\n",
      "Epoch [5/10], Step [30/48], Loss: 0.0331\n",
      "Epoch [5/10], Step [40/48], Loss: 0.0322\n",
      "34\n",
      "Accuracy of the network on the 50 test images: 68.0 %\n",
      "Epoch [6/10], Step [10/48], Loss: 0.0235\n",
      "Epoch [6/10], Step [20/48], Loss: 0.0185\n",
      "Epoch [6/10], Step [30/48], Loss: 0.0245\n",
      "Epoch [6/10], Step [40/48], Loss: 0.0278\n",
      "34\n",
      "Accuracy of the network on the 50 test images: 68.0 %\n",
      "Epoch [7/10], Step [10/48], Loss: 0.0039\n",
      "Epoch [7/10], Step [20/48], Loss: 0.0119\n",
      "Epoch [7/10], Step [30/48], Loss: 0.0118\n",
      "Epoch [7/10], Step [40/48], Loss: 0.0150\n",
      "35\n",
      "Accuracy of the network on the 50 test images: 70.0 %\n",
      "Epoch [8/10], Step [10/48], Loss: 0.0064\n",
      "Epoch [8/10], Step [20/48], Loss: 0.0134\n",
      "Epoch [8/10], Step [30/48], Loss: 0.0200\n",
      "Epoch [8/10], Step [40/48], Loss: 0.0257\n",
      "35\n",
      "Accuracy of the network on the 50 test images: 70.0 %\n",
      "Epoch [9/10], Step [10/48], Loss: 0.0087\n",
      "Epoch [9/10], Step [20/48], Loss: 0.0077\n",
      "Epoch [9/10], Step [30/48], Loss: 0.0127\n",
      "Epoch [9/10], Step [40/48], Loss: 0.0109\n",
      "34\n",
      "Accuracy of the network on the 50 test images: 68.0 %\n",
      "Epoch [10/10], Step [10/48], Loss: 0.0119\n",
      "Epoch [10/10], Step [20/48], Loss: 0.0069\n",
      "Epoch [10/10], Step [30/48], Loss: 0.0117\n",
      "Epoch [10/10], Step [40/48], Loss: 0.0177\n",
      "34\n",
      "Accuracy of the network on the 50 test images: 68.0 %\n",
      "模型训练完成\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "n_total_steps = len(train_loader)\n",
    "LossList = [] # 记录每一个epoch的loss\n",
    "AccuryList = [] # 每一个epoch的accury\n",
    "for epoch in range(num_epochs):\n",
    "    # -------\n",
    "    # 开始训练\n",
    "    # -------\n",
    "    model.train() # 切换为训练模型\n",
    "    totalLoss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device) # 图片大小转换\n",
    "        labels = labels.to(device)\n",
    "        # 正向传播以及损失的求取\n",
    "        outputs = model(images)\n",
    "#         labels = labels.squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        totalLoss = totalLoss + loss.item()\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad() # 梯度清空\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 权重更新\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, n_total_steps, totalLoss/(i+1)))\n",
    "    LossList.append(totalLoss/(i+1))\n",
    "    # ---------\n",
    "    # 开始测试\n",
    "    # ---------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1) # 预测的结果\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(correct)\n",
    "        acc = 100.0 * correct / total # 在测试集上总的准确率\n",
    "        AccuryList.append(acc)\n",
    "        print('Accuracy of the network on the {} test images: {} %'.format(total, acc))\n",
    "print(\"模型训练完成\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# model.train()\n",
    "batch_idx = 1\n",
    "epoches = 200\n",
    "for epoch in range(epoches):\n",
    "    batch_idx = 1\n",
    "    model.train()\n",
    "    for line in train_loader:\n",
    "        data = line[0].to(device)\n",
    "        label = line[1].to(device)\n",
    "        output = model(data)\n",
    "    #     print(output.size())\n",
    "    #     print(label.size())\n",
    "#         label = label.squeeze(1)\n",
    "        output = torch.log_softmax(output,dim = 1)\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad() #这一步可太TM重要了\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(loss)\n",
    "        if(batch_idx)%10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        batch_idx += 1\n",
    "        \n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    for line in test_loader:\n",
    "        data = line[0].to(device)\n",
    "        label = line[1].to(device)\n",
    "        output = model(data)\n",
    "        output = torch.log_softmax(output,dim = 1)\n",
    "        pred = output.max(1, keepdim=True)[1]# 找到概率最大的下标\n",
    "        correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    print(\"测试集上准确率为:\",(correct/50)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
